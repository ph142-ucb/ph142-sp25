---
title: "Flavors of T"

---

<!-- libraries -->
```{r,include=FALSE,purl=FALSE}
library(knitr) # for include_graphics() 
library(dplyr)
```

### Example with heights from two different populations

Are the heights of US and Dutch-born men different?

```{r heights-men, fig.align='center', out.width="80%", warning=F, message=F, echo=F}
library(tidyverse)
set.seed(123)
dutch <- rnorm(n = 100, mean = 183, sd = 7.4)
usa <- rnorm(n = 100, mean = 175.5, sd = 7.4)

height_data_wide = data.frame(usa, dutch)
height_data <- data.frame(height = c(dutch, usa), country = c(rep("Dutch", 100), rep("USA", 100)))

height_data %>% group_by(country) %>% summarise(sample_mean = mean(height), 
                                                sample_sd = sd(height), 
                                                length = length(height))
```


### Example with heights from two different populations
```{r men2, fig.align='center', out.width="80%", warning=F, message=F, echo=F}
ggplot(height_data, aes(x = height)) + 
  geom_histogram(aes(fill = country), binwidth = 5, col = "black") +
  theme_minimal(base_size = 15) +
  facet_wrap(~country, nrow = 2)
```

### Example with heights from two different populations
If we take out the last line of that code, the histograms will be plotted on one grid.  Notice what happens to the columns...
```{r men3, fig.align='center', out.width="80%", warning=F, message=F, echo=F}
ggplot(height_data, aes(x = height)) + 
  geom_histogram(aes(fill = country), binwidth = 5, col = "black") +
  theme_minimal(base_size = 15)
```

  
### Example with heights from two different populations  
```{r men4, fig.align='center', out.width="80%", warning=F, message=F, echo=F}
ggplot(height_data, aes(y = height)) + 
  geom_boxplot(aes(fill = country), col = "black") +
  theme_minimal(base_size = 15) 
```


## Two sample T test 


### Recall the one sample t-test!

$$\frac{\bar{x}-\mu}{s/\sqrt{n}}$$

We need to generalize this by replacing each piece in the t-test by the 
calculations on the previous slide:

The **two-sample** t-test is therefore:

$$t=\frac{(\bar{x}_1 - \bar{x}_2)-(\mu_1 - \mu_2)}{SE}$$

$$t=\frac{(\bar{x}_1 - \bar{x}_2)-(\mu_1 - \mu_2)}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}}$$
The two-sample t statistics has approximately a $t$ distribution. The approximation
is accurate when both sample sizes are greater than or equal to 5.

### Degrees of freedom for the two-sample t-test...

is bananas.

$$df = \frac{(\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2})^2}{\frac{1}{n_1-1}(\frac{s_1^2}{n_1})^2 + \frac{1}{n_2-1}(\frac{s_2^2}{n_2})^2}$$

Often this is approximated by assuming that the degrees of freedom is = $n_{1} - 1$ or $n_{2}-1$ whichever is smaller, 
or by making an assumption that the variance in the two samples is equal and approximating the df with $n_{1}+n_{2}-2$

We will NOT calculate df by hand - we will use R, or we will tell you to use one of the approximation methods.

### Hypothesis testing when you have two samples

$H_0: \mu_1-\mu_2=0$, obtain the **two-sample t-test** statistic

$$t=\frac{(\bar{x}_1 - \bar{x}_2)-(\mu_1 - \mu_2)}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}}$$

where the test **p-value** is the probability, when $H_0$ is true, of getting a
test statistics $t$ at least as extreme in the direction of $H_a$ as that obtained, 
and is computed as the corresponding area under the $t$ distribution with the
appropriate degrees of freedom.


### Example, continued

Let R do the work for you: 

```{r}
t.test(height_data_wide %>% pull(usa), 
       height_data_wide %>% pull(dutch), 
       alternative = "two.sided")
```

### Example, continued
Note that `t.test` gives you both the t-test results (t-statistic (called "t" in 
the output), df, and p-value), as well as the 95% CI. We got both because we 
performed a two-sided test.


### Example: Transgenic chickens

Infection of chickens with the avian flu is a threat to both poultry production
and human health. A research team created transgenic chickens resistant to avian
flu infection. Could the modification affect the chicken in other ways? The 
researchers compared the hatching weights (in grams) of 45 transgenic chickens
and 54 independently selected commercial chickens of the same breed.

```{r, echo=FALSE}
transgenic <- c(38.8, 39.0, 39.7, 40.0, 40.8, 40.9, 41.0, 41.0, 41.0, 42.5, 42.6, 43.0,
                43.0, 43.4, 43.5, 43.5, 43.8, 44.4, 44.7, 44.7, 44.7, 45.3, 45.7, 45.8, 
                46.4, 46.5, 46.6, 46.7, 46.7, 46.8, 46.9, 47.1, 47.1, 47.1, 47.3, 47.6,
                47.7, 48.1, 48.3, 49.3, 49.3, 49.8, 50.3, 50.9, 52.1)

commercial <- c(36.7, 37.1, 38.9, 39.5, 39.5, 39.8, 40.0, 40.2, 40.3, 40.5, 40.5, 40.7,
                41.1, 41.2, 41.5, 41.5, 41.6, 41.6, 41.7, 42.4, 43.1, 43.3, 43.3, 43.4,
                43.7, 44.1, 44.2, 45.2, 45.3, 45.4, 46.0, 46.1, 46.4, 46.6, 46.6, 46.9, 
                47.3, 47.5, 48.1, 48.2, 48.4, 48.6, 49.0, 49.1, 49.3, 49.6, 50.1, 50.2,  
                50.4, 50.6, 52.2, 53.0, 55.5, 56.4)

chicken_data <- data.frame(weight = c(transgenic, commercial), 
                           type = c(rep("transgenic", 45), rep("commercial", 54)))
```

### Example: Transgenic chickens
```{r, fig.align='center', out.width="80%", echo=F}
ggplot(chicken_data, aes(y = weight)) + geom_boxplot(aes(fill = type)) + theme_minimal(base_size = 15)

ggplot(chicken_data, aes(x = weight)) + geom_histogram(aes(fill = type), binwidth = 3, col = "black") +
  facet_wrap(~type, nrow = 2) + theme_minimal(base_size = 15)
```

### Estimate the size of the difference between the two means

```{r}
means <- chicken_data %>% 
  group_by(type) %>% 
  summarise(mean_weight = mean(weight))

diff_means <- means[1, 2] - means[2, 2]
diff_means
```

The estimated mean difference is -0.153 grams.

### Estimate the standard error

$SE = \sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}$

```{r}
chicken_stats <- chicken_data %>% 
  group_by(type) %>% 
  summarise(mean_weight = mean(weight),
            sd_weight = sd(weight), 
            n = length(weight)) 
```
 Use the output to calculate the SE:
 
$SE = \sqrt{\frac{4.568872^2}{54} + \frac{3.320836^2}{45}} = 0.7947528$

### Calculate the t-statistic

$$t=\frac{(\bar{x}_1 - \bar{x}_2)-(\mu_1 - \mu_2)}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}}$$
$$t=\frac{(44.98889 - 45.14222)-(0)}{ 0.7947528} = -0.1929279$$
What is the chance of observing the t-statistic -0.193 on the t-distribution 
with the appropriate degrees of freedom?

To answer this, we would need to calculate the degrees of freedom using that 
crazy formula. We won't do this. Instead, we will ask R to do the test for us 
(and verify that our calculated t-statistic matches R's test)

### t.test in R

Pay attention to the arguments specified by `t.test`. The first argument is the 
weight data for the commercial chickens and the second argument is the weight 
data for the transgenic chickens.

```{r, echo=FALSE}
commercial_weight <- chicken_data %>% filter(type == "commercial") %>% pull(weight)
transgenic_weight <- chicken_data %>% filter(type == "transgenic") %>% pull(weight)
```

```{r}
t.test(commercial_weight, transgenic_weight, alternative = "two.sided")
```


## CI for two sample t-test

### Confidence intervals for the two-sample t-test
For a one sample t-test the CI looked like this:

$$\bar{x}\pm t^*\frac{s}{\sqrt{n}}$$

When we have two samples it will look like this:

$$(\bar{x_1}-\bar{x_2}) \pm t^*\sqrt{\frac{s_1^2}{n_1}+{\frac{s_2^2}{n_2}}}$$ 

where $t^*$ is the critical value with area C between $-t^*$ and $t^*$ under the $t$
density curve with the appropriate degrees of freedom.


### R Recap 
We have done two types of t-test so far.  

A one sample t- test will take the form:

t.test(x = **x variable**, alternative = **greater, less or two.sided**, mu = **null hypothsis value**)

A two sample t-test will take the form:

t.test(**first sample data**, **second sample data**, alternative = **greater, less or two.sided**)
